# 🔥 Student Competence Analysis — with-Open-Source-AI-Models
*AI-powered feedback system for Python learning*  
*Author:* Akshat Shukla  
*Task 3 :* Evaluating Open Source Models for Student Competence Analysis

<img width="600" height="230" alt="image" src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRmxv4QqvKTYngv5f9WfjGdhikHDOFYNVxHmg&s" />

---

A complete, reproducible project for researching and prototyping automated competence analysis for Python learners using *CodeBERT* (analysis / embeddings) and *CodeLlama* (pedagogical prompt generation).  

This repository contains:  
📄 Research docs  
📊 Experimental notebooks  
💻 Student code datasets  
📑 Evaluation rubric  
📈 Visualizations  
📃 White paper  

---

## 📖 Table of Contents
- [Why this project](#why-this-project)  
- [Highlights / Features](#highlights--features)  
- [Repo layout (complete)](#repo-layout-complete)  
- [Quickstart — run everything locally](#quickstart--run-everything-locally)  
- [Design decisions & reproducibility notes](#design-decisions)  
- [Scoring Rubric (summary)](#scoring-rubric-summary)  
- [Limitations & ethical considerations](#limitations--ethical-considerations)  
- [Future work](#future-work)  
- [License & credits](#license--credits)  
- [Contact](#contact)  

---

## 🚀 Why this project
Large language models specialized for code can automate teaching tasks:  
- Detect syntax/logic errors  
- Identify misconceptions  
- Generate *pedagogical prompts* that encourage deeper learning  

This repo shows a *pipeline* combining:  
- ⚡ *CodeBERT* → precise error detection  
- 💡 *CodeLlama* → reflective, student-friendly feedback  

---

## ✨ Highlights / Features
- ✅ Research plan + reasoning + white paper (DOCX)  
- ✅ Beginner → Advanced student code datasets  
- ✅ Scripts & Jupyter notebooks for reproducible evaluation  
- ✅ JSON/Markdown scoring rubric + charts  
- ✅ Radar + bar visualizations of model performance  
- ✅ model_outputs.json with real examples  
- ✅ Hybrid system recommendation: CodeBERT (accuracy) + CodeLlama (pedagogy)  

---

## 📂 Repo layout (complete)

```text
student-competence-analysis-with-Open-Source-Models
├── README.md                      
├── research_plan.md
├── reasoning.md
├── CodeBERT_CodeLlama_WhitePaper.pdf
│   
├── models/
│   ├── codebert_notes.md
│   ├── codellama_notes.md
│   └── other_models.md
├── notebooks/
│   ├── code_analysis.ipynb
│   ├── prompt_eval.ipynb
│   └── model_evaluation.ipynb    # visual notebook (bar + radar charts)
├── scripts/
│   ├── evaluate_model.py
│   ├── utils.py
│   └── visualize_scores.ipynb        .ipynb recommended,.py optional
├── data/
│   ├── beginner_examples.py
│   ├── intermediate_examples.py
│   └── advanced_examples.py
├── results/
│   ├── model_outputs.json
│   ├── scoring_rubric.json
│   ├── scoring_rubric.md
│   └── figures/                  # PNGs generated by notebook
└── requirements.txt
```
---

## ⚡ Quickstart — run everything locally
```bash
# 1. clone
git clone <your-repo-url>
cd student-competence-analysis

# 2. setup environment
python -m venv .venv
source .venv/bin/activate   # Windows: .venv\Scripts\activate
pip install -r requirements.txt

# 3. run notebooks
jupyter notebook notebooks/model_evaluation.ipynb
```
---

## 📑 Design decisions & reproducibility notes

- Hybrid approach recommended: Use CodeBERT for large-scale static analysis (fast batch embedding + classifier) and CodeLlama for interactive hint generation.
- Normalization: We normalize variable names / whitespace to reduce superficial differences when using embeddings.
- Prompt-engineering: For CodeLlama, always ask the model to hint (Socratic style) not to reveal full code. 
- Human-in-the-loop: All automated feedback should be periodically spot-checked by instructors and used as an aid, not as final grading.



---

## 📊 Scoring Rubric (summary)
The project uses a 0–5 rubric with three criteria:

- Accuracy: Detects real code errors or misconceptions.
- Interpretability: Is feedback student-friendly and clear?
- Pedagogical Value: Does feedback encourage deeper thinking instead of giving the solution?



---

## ⚖️ Limitations & ethical considerations

- Hallucinations and overconfident answers occasionally occur; models can miss reasoning nuance.
- Resource constraints: Large CodeLlama variants need GPUs. Plan costs before deployment.
- Pedagogical risk: Over-reliance on AI for grading/feedback can reduce instructor oversight. Use the system as a scaffold.
- Privacy: If you collect student submissions, ensure consent & data protection (FERPA/GDPR as applicable).


---

## 🔮 Future work

- Fine-tune CodeLlama on real student error corpora to improve hint precision.
- Add more SOTA comparative baselines (e.g., StarCoder, GPT-NeoX) while preserving open-source deployment.
- Run user studies with students & instructors to measure learning impact.
- Build an API + front-end demo with interactive hinting and teacher dashboards.

---

## 📜 License & Credits

License: MIT — see LICENSE

Author: Akshat Shukla

Credits: Hugging Face, Meta AI (CodeLlama), Microsoft (CodeBERT)



---

## 📧 Contact

##### 👤 Akshat Shukla
##### 🔗 GitHub: akkiyolo
##### 📩 Email: akshatshukla069@gmail.com


---





